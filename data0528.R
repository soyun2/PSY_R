#인공지능 > 머신러닝(기계학습) > 딥러닝(인공신경망)
#1.지도학습
#- 훈련데이터와 정답을 가지며 데이터를 분류/예측하는 함수를 만들어내는 기계학습.
#- 데이터분류(레이블링) 작업에 많은 비용과 시간이 소요.
#ㄱ. 분류(Classfication)
#   -데이터가 범주형 변수인 경우를 예측
#   -데이터에 label을 할당하여 분류
#   -label 2개 : 이진분류, 3개이상 : 다중 클래스 분류
#   -KNN(K-Nearest Neighbors), 나이브 베이즈, 결정트리, 서포트 벡터머신, 아프리오 알고리즘
#ㄴ. 회귀(Regression)
#   -연속값을 예측
#   -선형회귀, 신경망 
#2.비지도학습
#   -정답없이 훈련데이터만 가지고 데이터로부터 숨겨진 패턴을 탐색한느 기계학습.
#ㄱ. 클러스터링 : 특정 기준에 따라 유사한 데이터 사례들을 하나의 세트로 그룹화하고 고유한 패턴을 찾기위해 개별 그룹 차원에서 분석 수행
#   -연관규칙(apriori) : 대규모 거래 데이터로부터 함께 구매될 규치긍ㄹ 도출하여 고객이 특정 상품 구매 시 이와 연관성이 높은 상품을 추천하는것.
#   -K-평균 알고리즘(K-means clustering algorithm) : 주어진 데이터를 K개 클러스터로 묶고 각 클러스터와 거리차이의 분산을 최소화하는 알고리즘.
#ㄴ. 차원 축소(Dimension Reduction)
#3.  강화학습(Reinforcement Learning)
#   -어떤 환경에서 정의된 에이전트가 현재 상태를 인식하여 선택 가능한 행동들 중에서 보상을 최대화 하는 행동 혹은 행동 순서를 선택하는 방법.
#   -알파고
#--------------------------------------------------------------------------------------------------------------------------------------------------
#1.의사결정나무(Decision tree, tree model)
#   -의사결정규칙을 나무 구조로 나타내어 전체 자료를 몇개의 소집단으로 분류(Classfication)하거나 예측(Prediction)을 수행하는 분석
#   -특정 조건을 기준으로 o/x로 나누어 분류/회귀를 진행하는 Tree구조의 분류/회귀 데이터마이닝 기법.
#   -종속변수가 범주형이면 Decision Tree Classfication으로 분류를 진행.
#   -종속변수가 연속형이면 Decision Tree Regression으로 회귀를 진행.
#   -고객 구매데이터(X)를 이용해 고객등급(Y)을 분류.
#   -야구 선수의 데이터(X)를 이용해 다음 해의 연봉(Y)을 예측하는 등의 상황에 활용.
#   -root node : 최상위 노드
#   -terminal node : 자식 노드가 없는 가장 아래 노드
#   -internal node : 자식이 존재하는 노드
#   -의사결정나무 목표는 최대한 균일한/순수한 상태의 부분집합을 찾는것.
#순수도(Purity) : 데이터들의 동질성이 높은 정도.
#   -데이터들의 동질성이 높은정도.
#   -부모 노드보다 더 순수도가 높은 자식 노드를 만들고 이를 반복하여 의사결정나무를 생성.
#예측 의사결정 나무모형
#   -조건에 따라 구역을 나눈 뒤 구역에 존재한 값들의 평균을 예측값으로 사용.
#구역을 나누는 방법은 RSS를 최소로 만드는 구역으로 나눔.
#RSS : 잔차 제곱합
#잔차 : 예측값과 관측한 실제값의 차이.
#   -j개의 구역을 나눌 수 있는 경우의 수는 무수히 많기 때문에 우리는 구역을 2개씩 연속적으로 나누어 가는 Recursive binary splitting 방법 활용.
#root node : 3, 4, 5, 7, 8, 9
#평균 : (3+4+5+7+8+9) / 6 = 6
#RSS : (6-3)^2 + (6-4)^2 + (6-5)^2 + (7-6)^2 + (8-6)^2 + (9-6)^2 = 28
#좌측 노드 : 3, 9, 7 = RSS:2
#우측 노드 : 5, 4, 9 = RSS:2
#▶ 분류 후에 어떻게 예측할것인가 선택
#   -기준에 따라서 구역을 나눈 뒤 각 구역에 존재하는 값들 중 최빈값(빈도수가 높은 값)을 예측 값으로 사용.
#▶ Gini index, Entropy
#   -의사결정나무는 하위 노드에서 종속변수를 가장 균일하게 존재하도록 만들어주는 x변수를 찾아서 반복적으로 가지를 뻗어 나가는 방법
#     -> 각노드의 균일성을 계산
#    노드 균일성을 계산하는 방법 : Gini index, entropy

#▶ Gini index(지니 계수)
#   -어떤 노드의 구성이 얼마나 순수한지를 계산한 값.
#   -고양이, 고양이, 고양이, 고양이, 강아지, 고양이, 고양이
#    (6/7) * (1/7) + (1/7) * (6/7) = 0.2449
#Gini index가 0이면 모든 데이터가 균일한(동일한) 데이터

#▶ Entropy
#   -데이터가 얼마나 무질서한지 나타냄
#   L node : 파랑3, 빨강3 => 빨강 50%, 파랑 50%
#   -(0.5 * log0.5 + 0.5 * log0.5) = 1
#   R node : 빨강6, 파랑0 => 빨강 100%, 파랑 0%
#   -(1.0 * log1 + 0 * log0) = 0
#   값들의 개수가 같으면 Entropy가 가장 높은 1
#   동일한 값들로 구성되어 있으면 Entropy가 0

#▶ IG(Information Gain) - 정보이득
#   Entropy를 이용해서 특정 x변수에 의해서 분화가 될 경우 얻게되는 정보의 양
#   가장 높은 IG값을 주는 x변수로 분화시켜서 tree model을 만드는게 목표

#▶ 과적합(Overfitting)
#   -머신러닝에서 학습 데이터를 너무 과하게 학습하는 것.
#   -가지치기(Pruning)를 사용하여 과적합 해결.

#▶ Tree Pruning(가지치기)
#   -최대한으로 학습된 의사결정 나무는 leaf node에서 항상 100% 균일성을 가짐 -> 과적합(overfitting) 발생

#▶ Pre-pruning
#   -tree 생성을 사전에 중단하는 방법.
#   -tree의 최대 깊이를 제한하여 leaf node의 최대 갯수를 제한하는 방법.

#▶ Post-pruning
#   -tree를 만든 후 하위 노드를 제거하거나 병합하여 데이터의 개수가 적은 노드를 삭제.